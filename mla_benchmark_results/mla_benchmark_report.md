# Multi-Latent Attention (MLA) Benchmark Report

## Overview

This report compares different MLA configurations across various dimensions:
- Different attention implementations (naive, absorb, naive+flash)
- Various head dimensions (rope, nope, value)
- Different numbers of heads
- Various batch sizes and sequence lengths

## Performance Metrics

- Latency (ms)
- Throughput (tokens/second)
- Peak memory usage (MB)

